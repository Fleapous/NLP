{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6903412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from word2number import w2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f4df7-6ae8-4b6e-9f34-4cf0fa0a4fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9b2c6-7079-4af6-a7b7-7c3e5866027a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf864890",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(\"scraped_data_latest.csv\")\n",
    "articles_df.fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "858712fb-6fa8-445d-878b-a06eceaece1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>update_date</th>\n",
       "      <th>meta_location</th>\n",
       "      <th>title</th>\n",
       "      <th>HTML_text</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>article_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-30 14:45:00</td>\n",
       "      <td>2024-03-30 15:03:00</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>Wheels of hazard: Motorcycle safety crisis unf...</td>\n",
       "      <td>&lt;p&gt;In Bangladesh, motorcycles, with their ease...</td>\n",
       "      <td>In Bangladesh, motorcycles, with their ease of...</td>\n",
       "      <td>https://www.unb.com.bd/category/Special/wheels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-19 10:36:00</td>\n",
       "      <td>2024-02-19 12:49:00</td>\n",
       "      <td>Gazipur</td>\n",
       "      <td>3 dead as truck hits autorickshaw in Gazipur’s...</td>\n",
       "      <td>&lt;p&gt;Three people were killed and two others inj...</td>\n",
       "      <td>Three people were killed and two others injure...</td>\n",
       "      <td>https://www.unb.com.bd/category/Bangladesh/3-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-15 11:43:00</td>\n",
       "      <td>2024-02-15 13:23:00</td>\n",
       "      <td>Sylhet</td>\n",
       "      <td>Six cops injured in road accident during vehic...</td>\n",
       "      <td>&lt;p&gt;During a routine vehicle inspection on the ...</td>\n",
       "      <td>During a routine vehicle inspection on the Syl...</td>\n",
       "      <td>https://www.unb.com.bd/category/Bangladesh/six...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-11 09:49:00</td>\n",
       "      <td>2024-02-11 12:12:00</td>\n",
       "      <td>Chattogram</td>\n",
       "      <td>Reckless driving triggers multi-vehicle collis...</td>\n",
       "      <td>&lt;p&gt;In a distressing incident last night, reckl...</td>\n",
       "      <td>In a distressing incident last night, reckless...</td>\n",
       "      <td>https://www.unb.com.bd/category/Bangladesh/rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-09 18:59:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>Chattogram</td>\n",
       "      <td>Out of control truck smashes into two other ve...</td>\n",
       "      <td>&lt;p&gt;A truck driver's helper was killed and 10 o...</td>\n",
       "      <td>A truck driver's helper was killed and 10 othe...</td>\n",
       "      <td>https://www.unb.com.bd/category/Bangladesh/out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2018-10-07 22:28:00</td>\n",
       "      <td>2018-10-07 23:08:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>Limo crash at popular tourist spot in NY kills 20</td>\n",
       "      <td>&lt;p&gt;Local officials told the Times Union of Alb...</td>\n",
       "      <td>Local officials told the Times Union of Albany...</td>\n",
       "      <td>https://www.unb.com.bd/category/World/limo-cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2018-10-07 20:22:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Man killed in gas cylinder blast in Bogura</td>\n",
       "      <td>&lt;p&gt;Bogura, Oct 7 (UNB) – An auto-rickshaw driv...</td>\n",
       "      <td>Bogura, Oct 7 (UNB) – An auto-rickshaw driver ...</td>\n",
       "      <td>https://www.unb.com.bd/category/Bangladesh/man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2018-10-07 15:31:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Woman killed falling off motorbike in city</td>\n",
       "      <td>&lt;p&gt;The deceased was identified as Halima Begum...</td>\n",
       "      <td>The deceased was identified as Halima Begum, 2...</td>\n",
       "      <td>https://www.unb.com.bd/category/Bangladesh/wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2018-10-07 13:08:00</td>\n",
       "      <td>2018-10-07 13:30:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>Jabal-e-Noor pays Tk 10 lakh to 2 road crash v...</td>\n",
       "      <td>&lt;p&gt;The lawyer of the Jabal-e-Noor Paribahan ow...</td>\n",
       "      <td>The lawyer of the Jabal-e-Noor Paribahan owner...</td>\n",
       "      <td>https://www.unb.com.bd/category/Bangladesh/jab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2018-10-07 11:54:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Schoolgirl killed in C’nawabganj road crash</td>\n",
       "      <td>&lt;p&gt;The deceased was identified as Kabita, 15, ...</td>\n",
       "      <td>The deceased was identified as Kabita, 15, dau...</td>\n",
       "      <td>https://www.unb.com.bd/category/Bangladesh/sch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        publication_date          update_date meta_location  \\\n",
       "0    2024-03-30 14:45:00  2024-03-30 15:03:00         Dhaka   \n",
       "1    2024-02-19 10:36:00  2024-02-19 12:49:00       Gazipur   \n",
       "2    2024-02-15 11:43:00  2024-02-15 13:23:00        Sylhet   \n",
       "3    2024-02-11 09:49:00  2024-02-11 12:12:00    Chattogram   \n",
       "4    2024-02-09 18:59:00                   NA    Chattogram   \n",
       "..                   ...                  ...           ...   \n",
       "995  2018-10-07 22:28:00  2018-10-07 23:08:00            NA   \n",
       "996  2018-10-07 20:22:00                   NA            NA   \n",
       "997  2018-10-07 15:31:00                   NA            NA   \n",
       "998  2018-10-07 13:08:00  2018-10-07 13:30:00            NA   \n",
       "999  2018-10-07 11:54:00                   NA            NA   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Wheels of hazard: Motorcycle safety crisis unf...   \n",
       "1    3 dead as truck hits autorickshaw in Gazipur’s...   \n",
       "2    Six cops injured in road accident during vehic...   \n",
       "3    Reckless driving triggers multi-vehicle collis...   \n",
       "4    Out of control truck smashes into two other ve...   \n",
       "..                                                 ...   \n",
       "995  Limo crash at popular tourist spot in NY kills 20   \n",
       "996         Man killed in gas cylinder blast in Bogura   \n",
       "997         Woman killed falling off motorbike in city   \n",
       "998  Jabal-e-Noor pays Tk 10 lakh to 2 road crash v...   \n",
       "999        Schoolgirl killed in C’nawabganj road crash   \n",
       "\n",
       "                                             HTML_text  \\\n",
       "0    <p>In Bangladesh, motorcycles, with their ease...   \n",
       "1    <p>Three people were killed and two others inj...   \n",
       "2    <p>During a routine vehicle inspection on the ...   \n",
       "3    <p>In a distressing incident last night, reckl...   \n",
       "4    <p>A truck driver's helper was killed and 10 o...   \n",
       "..                                                 ...   \n",
       "995  <p>Local officials told the Times Union of Alb...   \n",
       "996  <p>Bogura, Oct 7 (UNB) – An auto-rickshaw driv...   \n",
       "997  <p>The deceased was identified as Halima Begum...   \n",
       "998  <p>The lawyer of the Jabal-e-Noor Paribahan ow...   \n",
       "999  <p>The deceased was identified as Kabita, 15, ...   \n",
       "\n",
       "                                              raw_text  \\\n",
       "0    In Bangladesh, motorcycles, with their ease of...   \n",
       "1    Three people were killed and two others injure...   \n",
       "2    During a routine vehicle inspection on the Syl...   \n",
       "3    In a distressing incident last night, reckless...   \n",
       "4    A truck driver's helper was killed and 10 othe...   \n",
       "..                                                 ...   \n",
       "995  Local officials told the Times Union of Albany...   \n",
       "996  Bogura, Oct 7 (UNB) – An auto-rickshaw driver ...   \n",
       "997  The deceased was identified as Halima Begum, 2...   \n",
       "998  The lawyer of the Jabal-e-Noor Paribahan owner...   \n",
       "999  The deceased was identified as Kabita, 15, dau...   \n",
       "\n",
       "                                           article_url  \n",
       "0    https://www.unb.com.bd/category/Special/wheels...  \n",
       "1    https://www.unb.com.bd/category/Bangladesh/3-d...  \n",
       "2    https://www.unb.com.bd/category/Bangladesh/six...  \n",
       "3    https://www.unb.com.bd/category/Bangladesh/rec...  \n",
       "4    https://www.unb.com.bd/category/Bangladesh/out...  \n",
       "..                                                 ...  \n",
       "995  https://www.unb.com.bd/category/World/limo-cra...  \n",
       "996  https://www.unb.com.bd/category/Bangladesh/man...  \n",
       "997  https://www.unb.com.bd/category/Bangladesh/wom...  \n",
       "998  https://www.unb.com.bd/category/Bangladesh/jab...  \n",
       "999  https://www.unb.com.bd/category/Bangladesh/sch...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193370dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_days = {'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78da6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db12849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bangladesh_divisions = pd.read_csv('bangladesh_divisions.csv', header=None)\n",
    "bangladesh_divisions = bangladesh_divisions.drop(columns=[0, 2, 3])\n",
    "bangladesh_divisions = bangladesh_divisions.rename(columns={1: 'division'})\n",
    "bangladesh_divisions.index = range(1, len(bangladesh_divisions) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d038054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bangladesh_districts = pd.read_csv('bangladesh_districts.csv', header=None)\n",
    "bangladesh_districts = bangladesh_districts.drop(columns=[0, 3, 4, 5, 6])\n",
    "bangladesh_districts = bangladesh_districts.rename(columns={1: 'division_idx', 2: 'district'})\n",
    "bangladesh_districts.index = range(1, len(bangladesh_districts) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7604c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "bangladesh_upazilas = pd.read_csv('bangladesh_upazilas.csv', header=None)\n",
    "bangladesh_upazilas = bangladesh_upazilas.drop(columns=[0, 3, 4])\n",
    "bangladesh_upazilas = bangladesh_upazilas.rename(columns={1: 'district_idx', 2: 'upazila'})\n",
    "bangladesh_upazilas.index = range(1, len(bangladesh_upazilas) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd193cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_of_places_of_accident = {\n",
    "    'plane': 10,\n",
    "    'airport' : 10,\n",
    "    'bus stand' : 10,\n",
    "    'crossing' : 6,\n",
    "    'intersection' : 6,\n",
    "    'bridge' : 6,\n",
    "    'tunnel' : 6, \n",
    "    'flyover' : 6,\n",
    "    'railroad' : 4,\n",
    "    'rail' : 4,\n",
    "    'highway': 3, \n",
    "    'expressway' : 3,\n",
    "    'bypass' : 3,\n",
    "    'road' : 1, \n",
    "    'water' : 1,\n",
    "}\n",
    "\n",
    "places_of_accident = list(priority_of_places_of_accident.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c148e7-f89d-414d-a9ee-543eec7a55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_set = {\n",
    "    \"oil tanker\",\n",
    "    \"road roller\",\n",
    "    \"power tiller\",\n",
    "    \"excavator\",\n",
    "    \"train\",\n",
    "    \"airplane\",\n",
    "    \"pedestrian\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"noah\",\n",
    "    \"human hauler\",\n",
    "    \"trolley\",\n",
    "    \"chander gari\",\n",
    "    \"gari\",\n",
    "    \"auto rickshaw\",\n",
    "    \"cng\",\n",
    "    \"easy-bike\",\n",
    "    \"truck\",\n",
    "    \"garbage truck\",\n",
    "    \"trailer\",\n",
    "    \"motorcycle\",\n",
    "    \"microbus\",\n",
    "    \"scooter\",\n",
    "    \"construction vehicle\",\n",
    "    \"bicycle\",\n",
    "    \"ambulance\",\n",
    "    \"pickup\",\n",
    "    \"lorry\",\n",
    "    \"paddy cutter vehicles\",\n",
    "    \"bulkhead\",\n",
    "    \"crane\",\n",
    "    \"wrecker\",\n",
    "    \"tractor\",\n",
    "    \"cart\",\n",
    "    \"leguna\",\n",
    "    \"nosimon\",\n",
    "    \"three-wheeler\",\n",
    "    \"four-wheeler\",\n",
    "    \"votvoti\",\n",
    "    \"kariman\",\n",
    "    \"mahindra\",\n",
    "    \"van\",\n",
    "    \"rickshaw\",\n",
    "    \"autorickshaw\",\n",
    "    \"boat\",\n",
    "    \"trawler\",\n",
    "    \"vessel\",\n",
    "    \"launch\",\n",
    "    \"tanker\",\n",
    "    \"motorcycles\",\n",
    "    \"motorbike\",\n",
    "    \"limousine\",\n",
    "    \"limo\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36da6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77624cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_trf = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "554e0bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "res_df = processArticles(articles_df, nlp_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0f2e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('processed_articles_all.csv', sep=';', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f6be3-d13d-41f4-98f3-66c999387ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08641dd2-df14-4c59-befc-bd54807df299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3987d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processArticles(articles_df, nlp_processor):\n",
    "    res_df = pd.DataFrame(columns=['<url>',\n",
    "                                   '<publication date metadata>',\n",
    "                                   '<location metadata>',\n",
    "                                   '<title>',\n",
    "                                   '<raw text>',\n",
    "                                   '<number_of_accidents_occured>',\n",
    "                                   '<is_the_accident_data_yearly_monthly_or_daily>', \n",
    "                                   '<day_of_the_week_of_the_accident>',\n",
    "                                   '<exact_location_of_accident>',\n",
    "                                   '<area_of_accident>',\n",
    "                                   '<division_of_accident>', \n",
    "                                   '<district_of_accident>', \n",
    "                                   '<subdistrict_or_upazila_of_accident>',\n",
    "                                   '<is_place_of_accident_highway_or_expressway_or_water_or_others>',\n",
    "                                   '<is_country_bangladesh_or_other_country>',\n",
    "                                   '<is_type_of_accident_road_accident_or_train_accident_or_waterways_accident_or_plane_accident>',\n",
    "                                   '<total_number_of_people_killed>',\n",
    "                                   '<total_number_of_people_injured>',\n",
    "                                   '<is_reason_or_cause_for_the_accident_ploughed_or_ram_or_hit_or_collision_or_breakfail_or_others>',\n",
    "                                   '<primary_vehicle_involved>',\n",
    "                                   '<secondary_vehicle_involved>',\n",
    "                                   '<tertiary_vehicle_involved>',\n",
    "                                   '<any_more_vehicles_involved>',\n",
    "                                   '<available_ages_of_the_deceased>',\n",
    "                                   '<accident_datetime_from_url>',\n",
    "                                  ])\n",
    "    \n",
    "    for index, article in articles_df.iterrows():\n",
    "        print(index)\n",
    "        processed = processArticle(article, nlp_processor)\n",
    "        for i in range(len(processed)):\n",
    "            processed[i] = '<' + processed[i] + '>'\n",
    "        res_df.loc[index] = processed\n",
    "        \n",
    "    return res_df\n",
    "\n",
    "def processArticle(article, nlp_processor):\n",
    "    text = article['title'] + '.\\n' + article['raw_text']\n",
    "    doc = nlp_processor(text)\n",
    "\n",
    "    # <number_of_accidents_occured>\n",
    "    accidents_num = determineNumOfAccidents(doc) # returns int\n",
    "    \n",
    "    # <is_the_accident_data_yearly_monthly_or_daily>\n",
    "    # <day_of_the_week_of_the_accident>\n",
    "    (mode, day) = determineTime(doc) # returns (srt, str)\n",
    "\n",
    "    # <division_of_accident>\n",
    "    # <district_of_accident>\n",
    "    # <subdistrict_or_upazila_of_accident>\n",
    "    (division, district, upazila) = determineRegion(doc) # returns (str, str, str)\n",
    "\n",
    "    # <is_place_of_accident_highway_or_expressway_or_water_or_others>\n",
    "    (place, place_token) = placeOfAccident(doc) # returns (str, spacy.token.Token)\n",
    "\n",
    "    # <exact_location_of_accident>\n",
    "    exact_loc = exactLocationOfAccident(place_token) # returns str\n",
    "\n",
    "    # <area_of_accident>\n",
    "    area = determineLocation(doc) # returns str\n",
    "    \n",
    "    # <total_number_of_people_killed>\n",
    "    total_fatalities = extractFatalities(doc) # returns int\n",
    "    \n",
    "    # <total_number_of_people_injured>\n",
    "    total_injuries = extractInjuries(doc) # returns int\n",
    "    \n",
    "    # <primary_vehicle_involved>\n",
    "    # <secondary_vehicle_involved>\n",
    "    # <tertiary_vehicle_involved>\n",
    "    # <any_more_vehicles_involved>\n",
    "    vehicles = findVehicles(doc) # returns list of str (max len = 4)\n",
    "    veh1, veh2, veh3, veh4 = 'NA', 'NA', 'NA', 'NA'\n",
    "    if len(vehicles) >= 1:\n",
    "        veh1 = vehicles[0]\n",
    "    if len(vehicles) >= 2:\n",
    "        veh2 = vehicles[1]\n",
    "    if len(vehicles) >= 3:\n",
    "        veh3 = vehicles[2]\n",
    "    if len(vehicles) >= 4:\n",
    "        veh4 = vehicles[3]\n",
    "    \n",
    "    # <available_ages_of_the_deceased>\n",
    "    ages = findAges(doc) # returns list of ints\n",
    "\n",
    "    # <is_country_bangladesh_or_other_country>\n",
    "    country = determineBangladeshOrOther(doc) # returns string\n",
    "    \n",
    "    # <is_type_of_accident_road_accident_or_train_accident_or_waterways_accident_or_plane_accident>\n",
    "    accident_type = determineAccidentType(doc) # returns str\n",
    "\n",
    "    # <accident_datetime_from_url>\n",
    "    accident_time = formatAccidentTime(article['publication_date']) # returns str\n",
    "\n",
    "    # <is_reason_or_cause_for_the_accident_ploughed_or_ram_or_hit_or_collision_or_breakfail_or_others>\n",
    "    reason = determineAccidentReason(doc) # returns str\n",
    "\n",
    "    return [article['article_url'], # <url>\n",
    "            article['publication_date'], # <publication_date_metadata>\n",
    "            article['meta_location'], # <location_metadata>\n",
    "            article['title'], # <title>\n",
    "            article['raw_text'], # <raw_text>\n",
    "            str(accidents_num), # <number_of_accidents_occured>\n",
    "            mode, # <is_the_accident_data_yearly_monthly_or_daily>\n",
    "            day, # <day_of_the_week_of_the_accident>\n",
    "            exact_loc, # <exact_location_of_accident>\n",
    "            area, # <area_of_accident>\n",
    "            division, # <division_of_accident>\n",
    "            district, # <district_of_accident>\n",
    "            upazila, # <subdistrict_or_upazila_of_accident>\n",
    "            place, # <is_place_of_accident_highway_or_expressway_or_water_or_others>\n",
    "            country, # <is_country_bangladesh_or_other_country>\n",
    "            accident_type, # <is_type_of_accident_road_accident_or_train_accident_or_waterways_accident_or_plane_accident>\n",
    "            str(total_fatalities), # <total_number_of_people_killed>\n",
    "            str(total_injuries), # <total_number_of_people_injured>\n",
    "            reason, # <is_reason_or_cause_for_the_accident_ploughed_or_ram_or_hit_or_collision_or_breakfail_or_others>\n",
    "            veh1, # <primary_vehicle_involved>\n",
    "            veh2, # <secondary_vehicle_involved>\n",
    "            veh3, # <tertiary_vehicle_involved>\n",
    "            veh4, # <any_more_vehicles_involved>\n",
    "            \"(\" + \", \".join([str(age) for age in ages]) + \")\", # <available_ages_of_the_deceased>\n",
    "            accident_time # <accident_datetime_from_url>\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d10c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineTime(doc):\n",
    "    mode = 'NA' # D / M / Y\n",
    "    day = 'NA' # day of the week\n",
    "    for i in doc:\n",
    "        if i.ent_type_ != 'DATE' and i.ent_type_ != 'TIME':\n",
    "            continue\n",
    "        if i.text in week_days:\n",
    "            mode = 'D'\n",
    "            day = i.text\n",
    "            break\n",
    "        elif i.text in months:\n",
    "            mode = 'M'\n",
    "        else:\n",
    "            if mode != 'M':\n",
    "                mode = 'Y'\n",
    "    return (mode, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd651b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineRegion(doc):\n",
    "    division = 'NA'\n",
    "    district = 'NA'\n",
    "    upazila = 'NA'\n",
    "    for i in doc:\n",
    "        if i.ent_type_ != 'GPE' and i.ent_type_ != 'LOC':\n",
    "            continue\n",
    "        if bangladesh_upazilas['upazila'].isin([i.text]).any():\n",
    "            upazila = i.text\n",
    "            break;\n",
    "        if bangladesh_districts['district'].isin([i.text]).any():\n",
    "            district = i.text\n",
    "        if bangladesh_divisions['division'].isin([i.text]).any():\n",
    "            division = i.text\n",
    "    return disambiguateNAs(division, district, upazila)\n",
    "\n",
    "\n",
    "def disambiguateNAs(division, district, upazila):\n",
    "    if upazila != 'NA':\n",
    "        filtered_upazilas = bangladesh_upazilas[bangladesh_upazilas['upazila'] == upazila]\n",
    "        if len(filtered_upazilas) == 1:\n",
    "            district_idx = filtered_upazilas['district_idx'].item()\n",
    "            district = bangladesh_districts.loc[district_idx]['district']\n",
    "            division_idx = bangladesh_districts.loc[district_idx]['division_idx'].item()\n",
    "            division = bangladesh_divisions.loc[division_idx]['division']\n",
    "    if district != 'NA':\n",
    "        division_idx = bangladesh_districts[bangladesh_districts['district'] == district]['division_idx'].item()\n",
    "        division = bangladesh_divisions.loc[division_idx]['division']\n",
    "    return (division, district, upazila)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68929936-a385-45f1-b483-47faa9baf083",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_matcher = Matcher(nlp_trf.vocab)\n",
    "patternAccident = [{\"LOWER\": {\"IN\": places_of_accident}, }] # \"DEP\": {\"REGEX\": \"[a-zA-Z]*obj\"}}\n",
    "accidents_matcher.add(\"AccidentType\", [patternAccident])\n",
    "\n",
    "def verb_of_object(tok):\n",
    "    verbs = []\n",
    "    for i in tok.ancestors:\n",
    "        if i.pos_ == \"VERB\":\n",
    "            verbs.append(i)\n",
    "    return verbs\n",
    "\n",
    "def verbs_in_sent(tok):\n",
    "    verbs = []\n",
    "    for i in tok.sent:\n",
    "        if i.pos_ == \"VERB\":\n",
    "            verbs.append(i)\n",
    "    return verbs\n",
    "\n",
    "def placeOfAccident(doc):\n",
    "    place = 'NA'\n",
    "    place_token = None\n",
    "    priority = -1\n",
    "    matches = accidents_matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        tok = doc[start]\n",
    "        verbs = verbs_in_sent(tok)\n",
    "        for verb in verbs:\n",
    "            if verb.lemma_ in [\"occur\", \"happen\", \"crash\", \"hit\", \"ram\", \"die\", \"lead\", \"kill\"]:\n",
    "                curr_place = tok.text.lower()\n",
    "                curr_prior = priority_of_places_of_accident[curr_place]\n",
    "                if curr_prior > priority:\n",
    "                    priority = curr_prior\n",
    "                    place_token = tok\n",
    "                    place = curr_place\n",
    "                elif curr_prior == priority:\n",
    "                    for child in tok.children:\n",
    "                        if child.ent_type_ in [\"GPE\", \"FAC\", \"LOC\"]:\n",
    "                            priority = curr_prior\n",
    "                            place_token = tok\n",
    "                            place = curr_place\n",
    "                            break\n",
    "    return place, place_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f74ed8c-1f61-4771-a242-10f2b148b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactLocationOfAccident(tok):\n",
    "    ret = 'NA'\n",
    "    if tok is not None:\n",
    "        if re.match(r\"[a-zA-Z]*obj\", tok.dep_):\n",
    "            sent = tok.sent\n",
    "            for chunk in sent.noun_chunks:\n",
    "                if chunk.root == tok:\n",
    "                    ret = chunk.text\n",
    "                    break\n",
    "        else:\n",
    "            idx = tok.i - 1\n",
    "            doc = tok.doc\n",
    "            l = [tok.text]\n",
    "            while(doc[idx].ent_type_ in [\"GPE\", \"FAC\", \"LOC\"]):\n",
    "                l.append(doc[idx].text)\n",
    "                idx -= 1\n",
    "            l.reverse()\n",
    "            ret = ' '.join(l)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec6c5868-7a1b-4fff-8ead-f10fc5a513d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFatalities(doc):\n",
    "    # Initialize variables to store the total number of people died and current sentence count\n",
    "    total_fatalities = 0\n",
    "    for sentence in doc.sents:\n",
    "        # Iterate through each token in the sentence\n",
    "        for token in sentence:\n",
    "            # Check if the token is a verb or adjective indicating death\n",
    "            if token.lemma_ in [\"die\", \"kill\", \"dead\", \"fatality\"] and token.pos_ in [\"VERB\"]:\n",
    "                # Get the subject of the verb (typically the person or people who died)\n",
    "                subject = [child for child in token.children if child.dep_ in [\"nsubj\", \"nsubjpass\", \"dobj\"]]\n",
    "                # If a subject is found\n",
    "                if subject:\n",
    "                    subject_text = subject[0].text\n",
    "\n",
    "                    # Check if the subject can be written as a number\n",
    "                    try:\n",
    "                        num_value = w2n.word_to_num(subject_text)\n",
    "                        total_fatalities += num_value\n",
    "\n",
    "                        return total_fatalities\n",
    "                    except ValueError:\n",
    "                        if any(child.dep_ == \"nummod\" for child in subject[0].children):\n",
    "                            nummod_child = [child for child in subject[0].children if child.dep_ == \"nummod\"][0]\n",
    "                            nummod_text = nummod_child.text\n",
    "                            # Check if the nummod has a compound children\n",
    "                            compound_child = [child for child in nummod_child.children if child.dep_ == \"compound\"]\n",
    "                            if compound_child:\n",
    "                                compound_text = \" \".join([child.text for child in compound_child] + [nummod_text])\n",
    "                                try:\n",
    "                                    total_fatalities += w2n.word_to_num(compound_text)\n",
    "                                    return total_fatalities\n",
    "                                except ValueError:\n",
    "                                    pass  # If conversion fails, continue to the next check\n",
    "                            else:\n",
    "                                try:\n",
    "                                    total_fatalities += w2n.word_to_num(nummod_text)\n",
    "                                    return total_fatalities\n",
    "                                except ValueError:\n",
    "                                    pass  # If conversion fails, continue to the next check\n",
    "                        elif any(child.dep_ == \"cc\" for child in subject[0].children):\n",
    "                            total_fatalities += 2\n",
    "\n",
    "                            return total_fatalities\n",
    "                        else:\n",
    "                            total_fatalities += 1\n",
    "\n",
    "                            return total_fatalities\n",
    "            elif token.lemma_ in [\"die\", \"kill\", \"dead\", \"fatality\"] and token.pos_ in [\"ADJ\"]:\n",
    "                # Check if the head token of the adjective is an auxiliary (AUX)\n",
    "                if token.head.pos_ == \"AUX\":\n",
    "                    # Get the subject of the aux\n",
    "                    subject = [child for child in token.head.children if child.dep_ in [\"nsubj\", \"nsubjpass\", \"dobj\"]]\n",
    "                    if subject:\n",
    "                        # Check if the nsubj has a nummod\n",
    "                        if any(child.dep_ == \"nummod\" for child in subject[0].children):\n",
    "                            nummod_child = [child for child in subject[0].children if child.dep_ == \"nummod\"][0]\n",
    "                            nummod_text = nummod_child.text\n",
    "                            # Check if the nummod has a compound children\n",
    "                            compound_child = [child for child in nummod_child.children if child.dep_ == \"compound\"]\n",
    "                            if compound_child:\n",
    "                                compound_text = \" \".join([child.text for child in compound_child] + [nummod_text])\n",
    "                                try:\n",
    "                                    total_fatalities += w2n.word_to_num(compound_text)\n",
    "                                    return total_fatalities\n",
    "                                except ValueError:\n",
    "                                    pass  # If conversion fails, continue to the next check\n",
    "                            else:\n",
    "                                try:\n",
    "                                    total_fatalities += w2n.word_to_num(nummod_text)\n",
    "                                    return total_fatalities\n",
    "                                except ValueError:\n",
    "                                    pass  # If conversion fails, continue to the next check\n",
    "                        # Check if the nsubj has a cc\n",
    "                        elif any(child.dep_ == \"cc\" for child in subject[0].children):\n",
    "                            total_fatalities += 2\n",
    "                            return total_fatalities\n",
    "                        else:\n",
    "                            total_fatalities += 1\n",
    "                            return total_fatalities\n",
    "\n",
    "    return total_fatalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c960ee1-18eb-4795-b229-6c56da0d64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractInjuries(doc):\n",
    "    # Initialize variables to store the total number of people injured\n",
    "    total_injuries = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        # Iterate through each token in the sentence\n",
    "        for token in sentence:\n",
    "            # Check if the token is a verb indicating injury\n",
    "            if token.lemma_ in [\"injure\", \"wound\", \"hurt\", \"sustained\"] and token.pos_ == \"VERB\":\n",
    "                # Get the subject of the verb (typically the person or people who were injured)\n",
    "                subject = [child for child in token.children if child.dep_ in [\"nsubj\", \"nsubjpass\", \"dobj\"]]\n",
    "\n",
    "                if subject:\n",
    "                    subject_text = subject[0].text\n",
    "\n",
    "                    # Check if the subject can be written as a number\n",
    "                    try:\n",
    "                        num_value = w2n.word_to_num(subject_text)\n",
    "                        total_injuries += num_value\n",
    "                        return total_injuries\n",
    "                    except ValueError:\n",
    "                        if any(child.dep_ == \"nummod\" for child in subject[0].children):\n",
    "                            nummod_child = [child for child in subject[0].children if child.dep_ == \"nummod\"][0]\n",
    "                            nummod_text = nummod_child.text\n",
    "                            # Check if the nummod has a compound child\n",
    "                            compound_child = [child for child in nummod_child.children if child.dep_ == \"compound\"]\n",
    "                            if compound_child:\n",
    "                                compound_text = \" \".join([child.text for child in compound_child] + [nummod_text])\n",
    "                                try:\n",
    "                                    total_injuries += w2n.word_to_num(compound_text)\n",
    "                                    return total_injuries\n",
    "                                except ValueError:\n",
    "                                    pass  # If conversion fails, continue to the next check\n",
    "                            else:\n",
    "                                try:\n",
    "                                    total_injuries += w2n.word_to_num(nummod_text)\n",
    "                                    return total_injuries\n",
    "                                except ValueError:\n",
    "                                    pass  # If conversion fails, continue to the next check\n",
    "                        elif any(child.dep_ == \"cc\" for child in subject[0].children):\n",
    "                            total_injuries += 2\n",
    "                            return total_injuries\n",
    "                        else:\n",
    "                            total_injuries += 1\n",
    "                            return total_injuries\n",
    "\n",
    "            # Specific handling for adjectives related to injuries\n",
    "            elif token.lemma_ in [\"injure\", \"wound\", \"hurt\", \"sustained\"] and token.pos_ == \"ADJ\":\n",
    "                # Check if the head token of the adjective is an auxiliary (AUX)\n",
    "                if token.head.pos_ == \"AUX\":\n",
    "                    # Get the subject of the aux\n",
    "                    subject = [child for child in token.head.children if child.dep_ in [\"nsubj\", \"nsubjpass\", \"dobj\"]]\n",
    "                    if subject:\n",
    "                        # Check if the nsubj has a nummod\n",
    "                        if any(child.dep_ == \"nummod\" for child in subject[0].children):\n",
    "                            nummod_child = [child for child in subject[0].children if child.dep_ == \"nummod\"][0]\n",
    "                            nummod_text = nummod_child.text\n",
    "                            # Check if the nummod has a compound child\n",
    "                            compound_child = [child for child in nummod_child.children if child.dep_ == \"compound\"]\n",
    "                            if compound_child:\n",
    "                                compound_text = \" \".join([child.text for child in compound_child] + [nummod_text])\n",
    "                                try:\n",
    "                                    total_injuries += w2n.word_to_num(compound_text)\n",
    "                                    return total_injuries\n",
    "                                except ValueError:\n",
    "                                    pass  # If conversion fails, continue to the next check\n",
    "                            else:\n",
    "                                try:\n",
    "                                    total_injuries += w2n.word_to_num(nummod_text)\n",
    "                                    return total_injuries\n",
    "                                except ValueError:\n",
    "                                    pass  # If conversion fails, continue to the next check\n",
    "                        # Check if the nsubj has a cc\n",
    "                        elif any(child.dep_ == \"cc\" for child in subject[0].children):\n",
    "                            total_injuries += 2\n",
    "                            return total_injuries\n",
    "                        else:\n",
    "                            total_injuries += 1\n",
    "                            return total_injuries\n",
    "\n",
    "    return total_injuries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "918c567a-a354-4838-a702-753f48e502e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAges(doc):\n",
    "    ages = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            last_token = ent[-1]\n",
    "\n",
    "            # Concatenate digits from the token text until a non-digit character is encountered\n",
    "            age_text = ''\n",
    "            for char in last_token.text:\n",
    "                if char.isdigit():\n",
    "                    age_text += char\n",
    "                else:\n",
    "                    if age_text:\n",
    "                        try:\n",
    "                            age = int(age_text)\n",
    "                            ages.append(age)\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                    age_text = ''\n",
    "\n",
    "            for child in last_token.children:\n",
    "                if child.dep_ == \"appos\":\n",
    "                    try:\n",
    "                        age = int(child.text)\n",
    "                        ages.append(age)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1e50a35-f8a2-4158-bc13-eacca52ba0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findVehicles(doc, max_depth=3):\n",
    "    vehicle_list = []\n",
    "    for token in doc:\n",
    "        if token.is_space and len(vehicle_list) != 0:\n",
    "            return vehicle_list\n",
    "\n",
    "        if token.text.lower() in vehicles_set:\n",
    "            current_token = token.head\n",
    "            for _ in range(max_depth):\n",
    "                if current_token.text.lower() in [\"crashed\", \"hit\", \"run\", \"lost\", \"ran\", \"collided\", \"collision\", \"smashed\", \"rolled\", \"crash\", \"crushed\"]:\n",
    "                    vehicle_list.append(token.text.lower())\n",
    "                    if len(vehicle_list) >= 4:\n",
    "                        return vehicle_list\n",
    "                    break\n",
    "                if current_token == current_token.head:\n",
    "                    break\n",
    "                current_token = current_token.head\n",
    "\n",
    "    return vehicle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2330dd84-c2a4-46b8-b344-ded81cbdc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineNumOfAccidents(doc):\n",
    "    accident_verbs = {\"occur\", \"happen\", \"crash\", \"hit\", \"ram\", \"die\", \"lead\", \"kill\"}\n",
    "    \n",
    "    seen_places = set()\n",
    "    count = 0\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        has_accident_verb = any(token.lemma_ in accident_verbs for token in sent if token.pos_ == \"VERB\")\n",
    "        \n",
    "        if has_accident_verb:\n",
    "            sentence_places = {entity.text for entity in sent.ents if entity.label_ in {\"GPE\", \"LOC\"}}\n",
    "            \n",
    "            if sentence_places - seen_places:\n",
    "                seen_places.update(sentence_places)\n",
    "                count += 1\n",
    "                \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92f28e71-d163-401e-b6ce-6723fc03645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineLocation(doc):\n",
    "    accident_verbs = {\"occur\", \"happen\", \"crash\", \"hit\", \"ram\", \"die\", \"lead\", \"kill\"}\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        has_accident_verb = any(token.lemma_ in accident_verbs for token in sent if token.pos_ == \"VERB\")\n",
    "        \n",
    "        if has_accident_verb:\n",
    "            for entity in sent.ents:\n",
    "                if entity.label_ in {\"GPE\", \"LOC\"}:\n",
    "                    return entity.text\n",
    "    (division, district, upazila) = determineRegion(doc)\n",
    "    if upazila != 'NA':\n",
    "        return upazila\n",
    "    if district != 'NA':\n",
    "        return district\n",
    "    if division != 'NA':\n",
    "        return division\n",
    "    return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edd3c73f-3e2a-4069-aa43-60694324581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineBangladeshOrOther(doc):\n",
    "    (division, district, upazila) = determineRegion(doc)\n",
    "    if(division != 'NA' or district != 'NA' or upazila != 'NA'):\n",
    "        return 'Bangladesh'\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db2cfa3e-9f76-40b8-b8c9-a9fb5b084c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineAccidentType(doc):\n",
    "    matcher = Matcher(nlp_trf.vocab)\n",
    "    accident_keywords = {\n",
    "        \"road\": [\"expressway\", \"crossing\", \"bypass\", \"bus stand\", \"bus stop\", \"parking\", \"bridge\", \"road\", \"intersection\", \"highway\", \"avenue\", \"drive\", \"street\", \"roundabout\", \"freeway\", \"motorway\", \"alley\"],\n",
    "        \"train\": [\"train\", \"railway\", \"railroad\", \"rail\", \"locomotive\", \"metro\", \"subway\", \"tram\", \"monorail\"],\n",
    "        \"plane\": [\"plane\", \"aircraft\", \"flight\", \"jet\", \"airline\", \"air\", \"airfield\", \"airport\" , \"aviation\", \"airbus\"],\n",
    "        \"waterways\": [\"boat\", \"ship\", \"vessel\", \"ferry\", \"waterway\", \"marine\", \"water\", \"yacht\", \"sailbaot\", \"cruise ship\", \"submarine\"],\n",
    "    }\n",
    "\n",
    "    for accident_type, keywords in accident_keywords.items():\n",
    "        pattern = [{\"LEMMA\": {\"IN\": keywords}}]\n",
    "        matcher.add(accident_type, [pattern])\n",
    "\n",
    "    match_count = {key: 0 for key in accident_keywords}\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        match_type = nlp_trf.vocab.strings[match_id]\n",
    "        match_count[match_type] += 1\n",
    "\n",
    "    determined_type = max(match_count, key=match_count.get)\n",
    "\n",
    "    if match_count[determined_type] == 0:\n",
    "        return \"other\"\n",
    "    \n",
    "    return determined_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af788650-e66c-47f5-87b8-b0591dc056f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatAccidentTime(publication_date):\n",
    "    dt = datetime.strptime(publication_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    formatted_date = dt.strftime(\"%Y%m%d %H:%M\")\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1106d7bc-427d-4fb8-aea4-9bec90f2ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_list = list(vehicles_set)\n",
    "vehicles_list.append('vehicle')\n",
    "reason_matcher = Matcher(nlp_trf.vocab)\n",
    "pattern_reason = [{\"LEMMA\": {\"IN\": vehicles_list}, }]\n",
    "reason_matcher.add(\"ReasonType\", [pattern_reason])\n",
    "\n",
    "def determineAccidentReason(doc):\n",
    "    matches = reason_matcher(doc)\n",
    "    verb = None\n",
    "    found = False\n",
    "    for match_id, start, end in matches:\n",
    "        tok = doc[start]\n",
    "        if re.match(r\"[a-zA-Z]*subj\", tok.dep_):\n",
    "            for anc in tok.ancestors:\n",
    "                if anc.pos_ == 'VERB':\n",
    "                    verb = anc\n",
    "                    found = True\n",
    "                    break\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "    if not verb:\n",
    "        return 'NA'\n",
    "    \n",
    "    prep = None\n",
    "    for tok in verb.subtree:\n",
    "        if tok.dep_ == 'prep' and tok.head == verb and tok.text not in {'on', 'in', 'at'}:\n",
    "            prep = tok\n",
    "    \n",
    "    res = 'NA'\n",
    "    if prep:\n",
    "        res = verb.lemma_ + ' ' + ' '.join([tok.text for tok in prep.subtree])\n",
    "    elif verb:\n",
    "        res = verb.lemma_\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d9d44-683e-488c-890c-e86f38bb1d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca576d5-a41b-4799-baf8-54c55a00c581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf5302-98b8-442a-936b-92a16f50c955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
